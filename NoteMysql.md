## Mysql事务

**1⃣️什么是事务和事务的作用是什么**

​                      a.整个操作是原子的，是一个最小的不可再分的工作单元。通常一个事务对应一个完整的工作单元(如银行的一次转账，电商的一次扣付款，删除一个人要删除它的所有相关信息)

​                      b.一个事务需要要一批DML(insert、update、delete)语句共同完成。事务只和dml语句有关。整个事务的dml语句要么不执行要么全部执行。

​                      c.在mysql中，只有使用innodb数据库引擎的数据库或表才能支持事务

**2⃣️事务需要满足的四个条件**

​                       1.原子性：整个事务的操作是一气呵成的，不可再分和中断

​                       2.一致性：一个事务里的dml语句要么都成功要么都失败

​                       3.隔离性：不同的事务之间互不干扰

​                          隔离性涉及的几个概念：

​                                                             a.脏读：指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是最终可能不会存到数据库中去。读到了最终可能不存在的数据

​                                                             b.可重复读：指的是在事务的最开始读到的数据和事务结束前的任意一刻读到的数据都是一致的。通常针对UPDATE操作

​                                                             c.不可重复读:与可重复读相反，在同一事务内，不同时刻读的同一数据是不一样的，可能是受到了事务内操作的影响。通常针对UPDATE操作

​                                                             d.幻读:针对INSERT操作。假设事务a对某些行的内容做了更改但还未提交，此时事务b插入了与事务a更改前的记录相同的记录行，并且在事务a提交之前提交了，此时会发现在事务a提交的修改对于某些数据无效，这些数据是事务b插入进来的。

​                           **隔离的几个级别(从左至右隔离性逐渐增强，性能逐渐变差)：**

​                           读未提交(READ UNCOMMITTED)--->读提交(READ COMMITTED)--->可重复读(REPEATABLE READ)--->串行性(SERIALIZABLE)

​                            只有串行化能够解决所有读的问题。mysql的事务隔离靠的是加锁来实现的，加锁自然会带来性能的损失

​							innodb引擎mysql默认事务隔离级别时可重复读

|       事务隔离级别        | 脏读 | 不可重复读 | 幻读 |
| :-----------------------: | :--: | :--------: | :--: |
| 读未提交(read-uncommited) |  是  |     是     |  是  |
|  读已提交(read-commited)  |  否  |     是     |  是  |
| 可重复读(repeatable-read) |  否  |     否     |  是  |
|   串行化(serializable)    |  否  |     否     |  否  |

​                         4.持久性：事务终结的标志，最终结果必须写到硬盘上    

**3⃣️事务处理的方法**

   *常规：*

​              begin或start transaction显式的开启一个事务。         

​      -----》

​               ROLLBACK中间执行到一半不满意可以回滚｜｜可增加保存点SAVE POINT,执行一次ROLLBACK会回到保存点
​      -----》

​               COMMIT执行最后一次提交

  *一般不用：改变set方式来改变mysql的自动提交模式：*

​               SET AUTOCOMMIT=0; 禁止自动提交 

​               SET AUTOCOMMIT=1; 开启自动提交          

  *注意事项，避免长事务的出现：*

​    1.事务随时可能会访问数据库里面的任何数据，因此在这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。除此之外还可能会占用锁资源，也可能拖垮整个库。

​    2.有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。因此尽量使用set autocommit=1, 通过显式语句的方式来启动事务。如果要避免二次交互也可以不使用commit，而是使用commit work and chain 语法提交。            

​    3.一些整理

- 首先，从应用开发端来看：

    1. 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。
    2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。
    3. 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）

- 其次，从数据库端来看：

      1. 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
      2. Percona 的 pt-kill 这个工具不错，推荐使用；
      3. 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
      4. 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

**4⃣️事务隔离性的实现**

1.行锁 见Mysql引擎版本的区别

​            如果一个事务在update、insert的时候，不能进行select，那么服务的并发访问性能就太差了。因此，我们日常的查询，都是“快照读”，不会上锁，只有在update\insert\“当前读”的时候，才会上锁。而为了解决“快照读”的并发访问问题，就引入了MVCC。

2.mvcc多版本控制

​            **什么是mvcc？**mvcc是乐观锁的一种实现方式，表中的一行记录在数据库中存了多个版本，每个版本以事务id作为版本号。innoDB里面每个事务有一个唯一的事务id，是在事务开始的时候向innoDB的事务系统申请的，并且按照申请顺序严格递增。假如一行数据被多个事务更新，就会产生多个版本的的记录。

​             **什么是一致性视图？**与数据库中view不同，一致性视图（consistent read view）定义了在事务期间能看到那些版本的数据

​                 读未提交(read-uncommited)	 没有一致性视图

​                 读已提交(read-commited)	 在每个sql开始执行的时候开始创建一致性视图

​                 可重复读(repeatable-read)  在事务开始时创建一致性视图(*实际使用发现事务a,b无论顺序，两个事务启动后,事务a不进行查询，事务b插入新数据并commit后，事务a可查询到该数据)

​                 串行化(serializable)	      不需要视图，直接通过加锁来避免一致性问题

​             **创建一致性视图的逻辑？**

​             名词：低水位：多个未提交的事务id组成的数组中，id最小的记录

​						 高水位：最大的id+1为高水位

​						 事务id数组：由事务id组成的。这个数组和高低水位组成了“一致性视图”

​              有了一致性视图后，我们就可以判断一行数据的多版本可见性了，无论是“读已提交”还是“可重复读”级别，可见性判断规则是一样的，区别在于创建快照（一致性视图）的时间。

	a.可见：这行数据的事务id小于低水位，此时事务已经提交了肯定可见
					版本号不在事务id数组中，且低于高水位，说明这个事务提交了肯定可见
					自己事务id内的任何变化都是可见的
	b.不可见：如果版本大于高水位，说明这行数据的这个事务id版本是在快照后产生的，肯定不可见
				   这行数据的版本号在事务id数组中，说明这个事务还没提交，不可见              

​                 结论：
​							a.mvcc利用“所有数据都有多个版本”的这个特性，实现了快速创建快照的能力

​						    b.mvcc的实现，就是利用当前事务id为依据创建“一致性视图”，利用一致性视图来判断数据版本的可见性

​                 **mvcc是如何解决幻读？**

​						单独的mvcc是不能解决幻读问题，这里引入Next-Key锁。每次锁住的不仅仅是索引数据，还包括这些数据附近的数据

​						Record Lock：单个行记录上的锁。

​						间隙锁：Gap Lock，锁定一个范围，但不包含记录本身。每当插入数据间隙都在变，间隙针对的是行与行之间数据还没插入的行。

​						Next-Key Lock：1+2，锁定一个范围，并且锁定记录本身。本质就是间隙锁加上行锁。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。

	*启动两个事务，可能出现两种阻塞的方式：
		1.事务B插入数据还未commit。此时事务A查询全量数据采用排他锁或者加Next-Key Lock范围落在事务b提交的索引上读的话会阻塞住，但若查询范围没落在事务b提交的索引上则不会给堵塞住。
		2.事务A全量数据加排他锁实时读或者Next-Key Lock读之后，往事务B插入数据该数据落在锁的范围内会造成阻塞
	*业务中间隙锁的使用
		1.间隙锁是在可重复读隔离级别下才会生效的，在这个级别下由于间隙锁的引入导致了同样的语句会锁住更大的范围，影响了服务的并发读。所以，如果把隔离级别设置为读提交的话，就没有间隙锁的问题了。但同时，要解决可能出现的数据和日志不一致的问题，需要把binlog格式设置为row。这也是不少公司使用的配置组合。
		2.在业务中运用不同的隔离级别必须要弄懂现象才可以确定使用

3.***<u>一致性读、当前读、行锁、两阶段协议概念连接</u>***

1. 在可重复读的隔离环境下，对于查询一个事务在创建快照之处到最后如果事务内没有任何增删改操作查询结果应该是一样的。
								     
2. 但对某行的修改操作实行的是先进行当前读，将最新事务修改并已经提交的该行数据读出来到当前事务id中去。当前读是需要加行锁的，如果两个事务ab,事务a要修改数据进行当前读，事务b提前先进行了修改但是还没有提交，由于两阶段锁协议，此时事务b没有返还行写锁，事务a就会阻塞住直到事务b提交。

3. 为什么当前读的时候，最新事务修改的提交要读到当前事务id中去？

      ​      因为快照是静态的，大于当前事务id提交的修改，对于当前事务来说是看不见的

5⃣️**Oracle 数据库的默认隔离级别其实就是“读提交”。**

​     对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。

6⃣️**逻辑备份**

​     1.mysql提供了一个全局锁，命令是 Flush tables with read lock (FTWRL)。

​        该锁可对整个数据库实例加锁，让其只能处于只读状态。

​        使用的典型场景就是全库的逻辑备份。

​        会造成两个问题，一个是由于在备份时期不能更新数据导致业务停摆；第二个是对从库进行备份，导致不能执行主库过来的binlog会造成主从延迟。

​	 2.假如不加锁进行逻辑备份会造成什么问题？

​	         可能会造成备份的库不是同一个逻辑时间点，视图不一致

​     3.官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。

​        该工具有什么不足？

​        只有innodb引擎是支持事务的，因此针对没有事务的引擎还是需要FTWRL    



# Mysql引擎版本的区别

1⃣️innodb支持事务，myisam不支持事务

2⃣️innodb支持行级锁，myisam支持表级锁

​       **表级锁：**开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的几率最大，并发度最低。表锁有两种，分别是表锁和元数据锁(meta data lock)。

​                       元数据锁跟innodb的意向锁是不同的，该锁位于server层。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。                                            

    例子：什么情况下给一个小表加个字段，导致整个库挂了？
       有顺序启动的四个事务ABCD。事务a读了某行数据占了mdl读锁，事务b也读某行数据顺利占了mdl读锁。事务c更新字段此时需要MDL写锁，但事物a还没有提交，因此事务c会阻塞住。当轮到事务d的时候无论是读还是写都没法进行下去，之后就会导致一堆的请求发进来后都堵住。
       以上情况是由于长事务导致的，因此对于占用时间过长的事务必须要约束住。例如改热门表的字段时候加等待时间过了等待时间就放弃更改
       在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。

​    **行级锁：**开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的几率最低，并发度最高。

什么是两阶段锁协议？***
        在innodb事务中，行锁是在需要的时候才加上的，但并不是操作完之后立即就返还锁，而是等到事务提交的时刻才释放。针对这个问题提醒我们，对于事务中需要锁多个行的情况，要把最有可能造成锁冲突，最可能影响并发度的往后放一下。

行锁出现了死锁要怎么办？

   1. 直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。但这个时间设置过长对在线服务是无法接受，时间过短会出现很多死锁解开的误伤。

   2. 主动死锁检测
            将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。默认情况下是开启的。每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

      ​      但是当大量的请求线程都更新同一行数据的时候，整体时间复杂度是O(N2),如果有1000个并发线程那么操作就是百万级别会消耗大量的cpu资源。

      ​       针对这个问题有几个方法就是：a.确保业务没有死锁、可以关掉这个配置
      ​                                                             b.通过一个消息队列控制并发度，增加一层中间件或者改数据库源码成本较高
      ​                                                             c.将单行通过业务设计分成多行，每次随机取一行更新分摊压力

​    **页面锁：**开销和加锁时间界于表级锁和行级锁止键；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般

  **InnoDB与MyISAM的最大不同有两点**：

  一是支持事务（TRANSACTION）；二是采用了行级锁。

*InnoDB实现了两种类型的行锁：*

​        a.共享锁(s)：允许一个事务去读一行。一个事务获取了一把共享锁的同时，也允许其他事务获得共享锁去读，但不能获取到排他锁。即一个事务在读取一个数据行的时候，其他事务也可以读，但不能对该数据行进行增删改。

​        b.排他锁(x):通常用于写操作。一个事务获取了一条数据行的排他锁，其他事务就不能获取该行的其他锁（包括共享锁和排他锁）。即一个事务在读取一个数据行的时候，其他事务不能对该数据行进行增删改查。

 *InnoDB的表锁：*

​      为了允许行锁和表锁共存，实现多粒度锁机制，innode内部还使用有两种意向锁。意向锁是都是系统自动添加和释放的，整个过程无需人工干预。

​        a.意向共享锁(is):通知数据库接下来需要施加什么锁并对表加锁。如果需要对记录A加共享锁，那么此时innodb会先找到这张表，对该表加意向共享锁之后，再对记录A添加共享锁。

​        b.意向排他锁(ix):通知数据库接下来需要施加什么锁并对表加锁。如果需要对记录A加排他锁，那么此时innodb会先找到这张表，对该表加意向排他锁之后，再对记录A添加共享锁。

​        InnoDB加锁：a.对于update、delete、insert语句,innodb会自动给涉及的数据集加上排他锁。

​                                 b.对于select语句，系统不会加任何锁，此时加锁需要自己显式的去加。

​                                                     共享锁（Ｓ）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE

​                                                     排他锁（X）：SELECT * FROM table_name WHERE ... FOR UPDATE

*InnoDB的间隙锁：*

​         加的也是共享锁或排他锁。当索引数据的键值是一个范围时，对于不存在的数据也会给其加上锁，这种锁机制就是所谓的间隙锁（gap锁）。

​         InnoDB使用间隙锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求。实际中一般不会使用。

3⃣️innodb支持外键，myisam不支持

​     主键、外键、索引：
​             a.主键:定义：唯一标示一条记录，不能有空不允许重复
​                                           作用：用来保证数据的完整性
​                                           个数：主键只能有一个
​             b.外键:定义：表的外键是另一表的主键，外键可以重复，也可以是空值。
​                                          作用：用来和另外一张表关联
​                                          个数：一张表可以有多个外键
​             c.索引：定义：该字段没有重复值，但可以有一个空值
​                                          作用：提高查询的速度
​                                          个数：一张表可以有多个唯一索引

4⃣️mysql还有一种特殊的引擎：MEMORY

​    它使用存储在内存中的内容来创建表，而且数据全部放在内存中。基于MEMORY的表的生命周期很短，一般是一次性的。



# Mysql的慢反应：

当响应时间过长时，mysql服务端会默认将反应时间较差的sql语句记录到日志中去。
              查看慢反应时间：show variables like 'long_query_time';
              查看慢反应记录日志的地址：show variables like 'slow_query%';

# Mysql的explain

explain显示了mysql如何使用索引来处理select语句以及连接表。可以帮助 选择更好的索引和写出更优化的查询语句。

MYSQL的profile: profile用来分析sql性能的消耗分布情况。当用explain无法解决慢sql的时候，需要用profile来对sql进行更细致的分析。找出sql所花的大部分时间消耗在那个部分，确认sql的性能瓶颈。
               set profiling=1; 打开分析   set progiling=0; 关闭分析
               show profiles;查看分析

# mysql的视图

定义：是一种虚拟的表，本身并不包含任何的数据，作为一个select语句保存在数据字典中。 

视图的作用：通过视图可以展示基表的部分数据。视图数据来自定义视图中的查询中使用的表，使用视图动态生成。
                       用来创建视图的表叫基表。改变基表的数据就可以变化视图的数据，也就是说视图通常用来读的

​        why: a.不需要关心背后的表结构、关联条件和筛选结构。对用户来说是已经过滤好的复合条件的结果集。

​                 b.使用视图可以将权限管理限制到某个行和某个列，用户只能访问他们被允许查询的结果集。

​                 c.一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名则可以通过修改视图来解决，不会对访问者造成影响。

创建语句：CREATE VIEW <视图名> AS <SELECT语句>

# mysql的删除

​     drop:直接删掉表；

​     truncate直接删掉表中的数据，再插入时自增长id又从1开始；

​     delete删除表中数据，可以加where子句。

​           为什么表数据删除，表大小不变？

​                 1. 删掉b+树上叶子结点的数据时候，并不会真正去删除而是把这个记录记为删除，如果之后再插入一条数据索引的顺序在这个位置时候就可以去复用。

​                 2. 这种删除分为两种情况，一种是单条记录被删除数据页其他记录还在，另一种是删掉整个数据页的记录。单条记录被删除如果复用必须索引的顺序在左右两个索引中间，而整个数据页被删掉记录则可以再索引顺序上的任何位置拿去复用。

​                 3. 这种删除可能会造成hole，包括插入数据也一样。正是这种没必要的hole导致了数据表空间一直占据着，因此mysql可以通过“重建表”达到收缩表空间的目的

​                 4. 重建表：类似与内存的碎片整理。另外建立一张backup表，按照原表主键id递增的顺序，把数据一行一行地从原表读出来再插入到backup表中去。

# mysql连接的种类

​               内连接：join或inner join：返回两边都有的

​               左连接:左连接是如果表A和表B查询的时候,先获得表A所有的数据,然后根据表A的每一条数据匹配B里的数据,如果没有,就列为空值(null)

​               右连接：与左连接正好相反,获取表B的数据,然后匹配A表的数据,如果没有的,列为空值(null)

​               交叉连接： select * from tableA cross join tableB 求表a表b的笛卡尔积

​               完整外部链接：不支持

# mysql的范式

​            第一范式(1NF)：数据库中的每一列都是不能重复的，每一列代表一个实体。不满足第一范式的数据库就不是关系型数据库

​            第二范式(2NF):要求所有非主键字段完全依赖主键，不能产生部分依赖

​            第三范式(3NF):

# MYSQL数据库优化

  1⃣️创建高性能的索引：根据业务设计符合业务的索引

  2⃣️查询性能优化：表的设计需要根据业务和范式设计一个好的表；使用得当的查询语句；

  3⃣️优化服务器的设置： 根据使用情况修改MySQL默认的配置文件；根据实际情况可做一些负载的设置；配置内存使用等等

# mysql的触发器

1⃣️触发器是与表有关的数据库对象，在满足定义条件时触发，并执行触发器中定义的语句集合。

2⃣️尽量少使用触发器，不建议使用

​      因为触发器的频率时针对每一行执行的，有几行就会触发几次，假如触发一次的时间长则十分消耗资源。

3⃣️触发器与存储过程的不同

​     触发器不同于存储过程，触发器主要是通过事件执行触发而被执行的，而存储过程可以通过存储过程名称名字而直接调用。

4⃣️可能需要使用的背景

​     通常通过创建触发器来强制实现不同表中的逻辑相关数据的引用完整性和一致性。由于用户不能绕过触发器，所以可以用它来强制实施复杂的业务规则，以确保数据的完整性。

# mysql的索引

1⃣️索引的本质

​     索引本身就是一种数据结构。在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。

2⃣️索引的存储分类

​     索引是在mysql的存储引擎层中实现的。有四种索引：

​              a.B-TREE索引：最常见的索引类型，大部分引擎都支持b树索引

​              b.HASH 索引：只有memory引擎支持

​              c.R-Tree 索引(空间索引)：空间索引是MyISAM的一种特殊索引类型，主要用于地理空间数据类型

​              d.Full-text (全文索引)：全文索引也是MyISAM的一种特殊索引类型，主要用于全文索引，InnoDB从MYSQL5.6版本提供对全文索引的支持。

3⃣️B-TREE索引类型

a.普通索引：最基本的索引类型，没有唯一性之类的限制。

b.UNIQUE索引：表示唯一的，不允许重复的索引。如果字段信息保证不会重复例如身份证信息等可设置为unique

c.主键：PRIMARY KEY索引，主键是一种唯一性索引，但它必须指定为“PRIMARY KEY”。每个表只能有一个主键。
              注：不能用CREATE INDEX语句创建PRIMARY KEY索引

4⃣️索引的语法

​       a.设置索引：

​          第一种alter table,用来创建普通索引，unique索引或primary key 索引

​                                        ALTER TABLE table_name ADD INDEX index_name (column_list)

​										ALTER TABLE table_name ADD UNIQUE (column_list)

​										ALTER TABLE table_name ADD PRIMARY KEY (column_list)

​		   第二种create index可对普通表增加普通索引或unique索引

​							            CREATE INDEX index_name on table_name (column_list)

​							            CREATE UNIQUE INDEX index_name on table_name (column_list)

​	    c.删除索引

​			可利用ALTER TABLE或DROP INDEX语句来删除索引

​		d.查看索引

​					          show index from table_name;

​					          show keys from table_name;

5⃣️索引选择

​         a.较频繁作为查询的字段应该作为索引

​         b.唯一性太差的字段不适合作为索引，即使查询频率很高

​         c.更新太频繁的字段不适合作为索引

​         d.不会出现在where子句中的字段不适合作为索引。悬着在where子句中出现的列，在join子句中出现的列作为索引

​         e.使用短索引，如果对字符串列进行索引，应该指定一个前缀长度，可节省大量索引空间，提升查询速度；

​                              前缀索引注意事项：使用了前缀索引就不能用覆盖索引去避免回表了

​                              技巧：1. 身份证号码前缀区分度低怎么办？ 

​                                          可以将身份证字段内所有字符串倒序存储

​                                          或者使用hash字段，再创建一个整数字段来保存身份证的校验码

6⃣️索引的注意事项

​    索引本身需要一定的空间去存储，mysql server维护索引需要消耗一定的系统资源。

​    以下两种情况一般不建议用索引： 

​                a.表记录比较少，如果只有几百条上千条数据只需做全表扫描即可。

​                b.索引列的重复数据太多，即选择性较低则不适合做索引。所谓索引的选择性（Selectivity），是指不重复的索引值（也叫基数，Cardinality）与表记录数（#T）的比值。

​                *需要注意的是mysql使用采样统计计算基数，所以这个基数很容易不准。

7⃣️索引的弊端

​     a.当有了索引之后，mysql实际维护了两块数据，一块是原表另一块是索引的数据。更新表中索引在原表中的列数据的同时，还需更新索引。(两棵树，一棵树索引-主键，另一颗主键-真实存储的数据)

​     b.多了一块索引，会造成存储空间资源的消耗

8⃣️索引的过程

​     1.假设有一个主键和一个单列普通索引

​              此时会先在普通索引树上先找到对应的叶子节点，该节点存着主键索引的值。查到后会跑到主键树上进行查询，取得对应的业务字段值，该过程称为回表。

​     2.通过覆盖索引避免回表

9⃣️普通索引和唯一索引该如何选择？

1. 前提：业务保证需要设立索引字段的值都不重复

2. 对于查询语句的影响？
       普通索引：查找到第一条满足条件的记录之后还需要继续往后找直到不满足

   ​	唯一索引：查找到第一条满足条件后就停止检索

   ​	由于innodb的数据是按数据页来读写到内存的，因此往后多找一次不影响整体资源消耗

3. 对于更新的影响？

   **change buffer**:  数据更改的缓冲队列，当更新一条数据的时候，如果该数据页在内存中就直接更新，如果不在内存中，在不影响数据一致性的前提下，innodb会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次需要读这个数据页的时候，将数据页读到内存中去，并且会执行change buffer中与这个页有关的操作。简单的说就是先不把修改写到磁盘中，等要读的时候再写。通过这种方式可以避免读数据到buffer pool占用的内存，同时加快更新语句执行成功返回的速度。

   **merge**: 表示change buffer中缓冲的操作，应用到原数据得到最新结果。这个过程可能是在读数据触发，或者是后台线程定期执行merge。在数据库正常关闭也会执行merge。

   唯一索引：插入一条数据，需要判断插入数据的索引是否满足唯一性约束。该操作必须要将数据页读入到内存中去。因此唯一性索引不回用到change buffer，当出现当量的插入数据的操作的时候，可能会使整个系统阻塞。

   普通索引：change buffer 只限于用在普通索引的场景下。

   ​		            写多读少：页面在写完之后马上被访问到的概率较小，此时效果最好。常见于账单类、日志类系统

   ​		            写入之后马上查询：此时随机访问io的次数不回减少反而加重了change buffer的负担。

 4. 如何选择？

       尽量使用普通索引。如果是写入之后马上查询的场景则关闭change buffer。对于数据量大的表的更新优化change buffer很明显。

5. change buffer 和redolog的区别

   a. 更新一条数据可能会出现两种情况。一种是在数据在内存中，那直接更新数据；另一种是数据在磁盘上，那就在change buffer区域记录下要更新的操作信息。

   b. 无论是以上那种情况，都会记录到redolog中去。

   c. 在更新完后读数据：如果是第一种数据在内存中，则直接会返回更新好正确的结果
    第二种需要进行merge, 即将磁盘中数据读出来，将change buffer中缓冲的操作，应用到原数据得到最新结果。

   d. redolog主要节省的是随机写磁盘的io消耗(转为顺序写)， changge buffer主要节省的是随机读磁盘的io消耗

🔟mysql优化器在自动选择索引的时候有什么策略？

		       1. 优化器选择索引通常是根据索引统计值(基数cardinality)、语句需要的扫描行、是否需要回表、查询排序等来决定
   		       2. 优化器选择的索引有时候可能会存在出错的情况，例如统计基数、扫描行出错。
   		       3.  改善方案是采用force index 强行选择一个索引或者是改善查询语句




MYSQL的联合索引和单列索引：1⃣️什么是联合索引:多个单列索引结合组成
                        2⃣️基础：1.最左匹配： 什么是最左匹配？
                                              如果sql语句中用到了联合索引最左边的的索引，那么这条语句就可以利用这个联合索引去进行匹配。当遇到(> 、 < 、 between、like)会自动停止匹配。
                                              例子(a,b): 可以匹配到索引的where条件 a=1 and b=2;
                                                                                a=1;
                                                                                b=2 and a=1;
                                                         不可以匹配到索引的where条件 b=2;
                                                  (a,b,c,d): 遇到c范围查询自动停止匹配 a=1 and b=2 and c>3 and d=0;
                                           最左匹配原理？
                                               通过联合索引建立的一颗b+树叶子结点的排序是按照第一个索引排序完之后再第一个索引的局部再进行第二个索引的排序，以此类推。
                                               因此，缺少最左边的索引单独把右边的索引去查询该索引是无序的，无序则代表此时索引失效了。
                                               另外，遇到左边索引范围查询之后，右边索引也不再是局部查询因此也无序了。
                        3⃣️不同情形如何建立索引：
                                1.SELECT * FROM table WHERE a = 1 and b = 2 and c = 3;
                                       此时需要看a,b,c三列的区分度，重复数据越少的字段越适合排在左边
                                2.SELECT * FROM table WHERE a > 1 and b = 2;
                                       (b,a) 范围查询会导致右边的索引失效
                                3.SELECT * FROM `table` WHERE a > 1 and b = 2 and c > 3;
                                       (b,a) or (b,c)看具体情况定
                                4.SELECT * FROM `table` WHERE a = 1 ORDER BY b;
                                        (a,b) 当a=1时，b相对有序可以避免再次排序
                                  SELECT * FROM `table` WHERE a > 1 ORDER BY b;
                                        (a) a的值是一个范围，这个范围内b是无序的，没有必要再对(a,b)建立索引
                                5.SELECT * FROM `table` WHERE a IN (1,2,3) and b > 1;
                                        (a,b) 此时a不是范围查询，in视为等值匹配，a是1，2，3里的一个值因此b还是局部有序



MYSQL的分库分表：1⃣️目的：当服务器io量、cpu使用率上升达到瓶颈的时候，连接数爆满了会造成服务器崩溃
                       io瓶颈：a.磁盘读io瓶颈，热点数据太多，缓存放不下，经常需要去磁盘读数据造成大量的io会造成查询速度下降----》分库和垂直分表
                              b.网络io瓶颈，请求的数据太多，网络带宽跟不上----》分库
                       cpu瓶颈: a.sql问题，查询时候存在一些像group by,order by, join, 非索引的条件查询等会造成cpu压力大。--》sql优化、建立索引、将计算放在业务服务端
                              b.单表数据量大，查询时候扫描的行太多,sql效率低，cpu出现瓶颈----》水平分表
               2⃣️分库分表的方式：
                         1.水平分库：
                               概念：按照一定策略（hash、range等），将一个库中的数据分到多个库中
                               结果：每个库的结构都一样；每个库的数据都不一样，没有交集；所有库的并集是全量数据
                               场景：系统绝对并发量上来了，分表难以从根本上解决问题，并且没有明显的业务归属来垂直分库。
                               结果：库多了机器多了，分摊到单个节点的的io和cpu压力就缓解了
                         2.水平分表：
                               概念：按照一定策略（hash、range等），将一个表中的数据分到多个表中
                               结果：每个表的结构都一样；每个表的数据都不一样，没有交集；所有表的并集是全量数据
                               场景：系统绝对并发量没有上来，只是单表的数据量太大了，影响了sql通过主键查询的效率，加重了cpu的负担，以至于成为了瓶颈
                               结果：表的数据量少了，单次执行sql的速度提高，减轻了cpu的压力
                         3.垂直分库:
                               概念：以表为依据，按照业务归属的不同，将不同的表拆分到不同的库中
                               结果：每个库的结构都不一样；每个库的数据也不一样，没有交集；所有库的并集是全量数据
                               场景：系统的并发量上来了，并且可以抽象出单独的业务模块
                               结果：随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。
                         4.垂直分表：
                               概念：以字段为依据，根据字段查询的热度，将表中字段拆到不同的表(主表和扩展表)去
                               结果：每个表的结构、数据都不一样；每个表的字段至少有一列是一样的，一般是主键，用于关联join
                               场景：系统绝对并发量没有上来，但是单行的字段多，热点数据和非热点数据在一起单行数据量过多，整体下来导致缓存过大，经常需要淘汰缓存数据导致查询时跑去db此时会导致大量的磁盘io
                               结果：拆分后单表存的更多的是热点数据，这帮助了缓存能够存更多的热点数据，减少了数据库的压力
              3⃣️拆分策略：
                    1.hash法：1.增加一个字段为有序的非主键索引id，整一个大的映射表通过映射表将单表的查询索引映射到该有序的非主键索引id。通过该id与表或库的数量取余，计算结果为需要分配到的节点，然后在该节点上查询就好了。
                              2.模仿reids的hash槽，通过crc16对查询索引转化出节点id

              4⃣️自增主键满了怎么办：
                         1.列数据类型增大至bigint，通常不推荐，表数据量太大了
                         2.分库分表，推荐
    
              5⃣️分库分表之后如何验证数据的一致性：
                         1.验证加起来后的数据量是否一致
                         2.只验关键性的几个字段是否一致
                         3.业务结果批量验证
    
              6⃣️分表分库的中间件: 
                          kingshard: https://github.com/flike/kingshard

MYSQL的四种语言：DDL,DML,DCL,TCL
         DDL（Data Definition Language）数据库定义语言statements are used to define the database structure or schema.
                CREATE(创建)、ALTER (修改)、DROP (删除)、TRUNCATE (清除)、COMMENT (注释)、RENAME (重命名)

         DML（Data Manipulation Language）数据操纵语言statements are used for managing data within schema objects. 需要commit
                 SELECT (查询)、INSERT (插入)、UPDATE (更新)、DELETE (删除)
                 MERGE (合并)(这个没用过.. 居然还是个引擎....)
                 CALL (调用存储过程) 先定义一个 存储过程,然后用call去调用这个过程..
                 EXPLAIN PLAN (性能分析)
                 LOCK TABLE (锁表)
         
        DCL（Data Control Language）数据库控制语言 授权，角色控制等
                 GRANT (授权)
                 REVOKE (取消授权)
    
        TCL（Transaction Control Language）事务控制语言
    			SAVEPOINT (设置保存点)
    			ROLLBACK (回滚)
    			SET TRANSACTION (设置事务)



悲观锁和乐观锁： 一种思路并不是真的锁。
        1⃣️乐观锁：乐观锁就是在操作数据的时候非常乐观，认为别人不会同时修改数据。因此乐观锁不会上锁，只是在执行更新的时候判断一下在此期间别人是否修改了数据b；如果别人修改了数据则放弃操作，否则执行操作。
           
           悲观锁：悲观锁在操作数据时认为别人会同时修改数据，因此直接将数据锁上，待操作完成在释放掉。
                     例子：select * from table where id = {0} for update 
                        先加排他锁锁住该行。直到事务提交或回滚时才会释放排它锁；在此期间，如果其他线程试图更新该玩家信息或者执行select for update，会被阻塞。
    
        2⃣️实现方式：悲观锁的方式是加锁，可能是对代码块加锁，或是对数据加锁(如MySQL中的排它锁)
                乐观锁的方式：1.CAS(Compare And Swap) 该操作是在cpu层面的原子操作
                                 三个操作数： a.需要读写的内存位置(V)
                                            b.需要比较的预期值(A)
                                            c.拟写入的新值(B)
                                 逻辑：如果内存位置V的值等于预期的A值，则该位置更新为新值B,否则不进行任何操作。许多CAS的操作是自旋的：如果操作不成功，会一直重试，直到操作成功为止。


​                                 

                            2.版本号机制（mysql的mvcc）
                                 给数据行增加一个版本号，每当数据被修改的时候，版本号会加1。当某个线程查询数据的时候，会将该版本号一起查询出，当该线程更新数据的时候，会判断当前版本号与之前读取的版本号是否一致，如果一致才进行操作。实际上可以根据实际情况选用其他能够标记数据版本的字段，如时间戳等。
    
                                 例子：select coins, level, version from player where player_id = xx; 查询出版本号
                                      update player set coins = {0}, version = version + 1 where player_id = xx and version = s;插入的时候判断下版本号
    
        3⃣️功能限制：1.CAS:只能保证单个变量的原子性，当涉及到多个变量的时候，CAS是无能为力的。而悲观锁可以加到整个代码块
                   2.版本号：如果query的时候针对的是表1，update的时候针对的是表2就没办法了。
    
        4⃣️适用场景：如果悲观锁和乐观锁都可以使用，那么选择就要考虑竞争的激烈程度：
    			当竞争不激烈 (出现并发冲突的概率小)时，乐观锁更有优势，因为悲观锁会锁住代码块或数据，其他线程无法同时访问，影响并发，而且加锁和释放锁都需要消耗额外的资源。
    			当竞争激烈(出现并发冲突的概率大)时，悲观锁更有优势，因为乐观锁在执行更新时频繁失败，需要不断重试，浪费CPU资源。
        
        5⃣️其他：1.乐观锁不加锁
               2.CAS的缺点：可能会出ABA问题，两个线程修改一个变量。对于ABA问题，比较有效的方案是引入版本号，内存中的值每发生一次变化，版本号都+1； 
                           出现自旋的时候可能造成cpu占用，可以引用退出机制，重试超过一定次数退出循环。

MYSQL的日志类型：
        1⃣️错误日志: 记录在启动，运行或停止mysqld时遇到的问题
        2⃣️通用查询日志：记录建立的客户端连接和执行的语句
        3⃣️二进制日志：记录更改数据的语句
        4⃣️中继日志：从服务器复制主服务器接收的数据更改
        5⃣️慢查询日志：记录所有执行时间超过long_query_time 秒的所有查询或者不使用索引的查询
        6⃣️ddl日志(元数据日志)：元数据操作由DDL语句执行

mysql的binlog
         MySQL 的二进制日志 binlog 可以说是 MySQL 最重要的日志，它记录了所有的 DDL 和 DML 语句（除了数据查询语句select、show等），以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的。binlog 的主要目的是复制和恢复。

         主要作用：
                复制:MySQL Replication在Master端开启binlog，Master把它的二进制日志传递给slaves并回放来达到master-slave数据一致的目的
    			数据恢复: 通过mysqlbinlog工具恢复数据
    			增量备份
         binlog管理：
                show variables like '%log_bin%';  查看配置文件或是否开启等相关信息
                show binary logs; //查看binlog文件列表,
                show master status; //查看当前二进制日志文件的状态信息
                reset master 清空binlog日志文件
         binlog内容查看：
                1.binlog工具 mysqlbinlog
                2.客户端命令后解析
                    SHOW BINLOG EVENTS  
                    [IN 'log_name'] //要查询的binlog文件名
                    [FROM pos]  
      				[LIMIT [offset,] row_count]
        binlog日志：
                可通过my.cnf配置文件 或者 set global binlog_format='ROW/STATEMENT/MIXED'； 进行修改
                show variables like 'binlog_format' 命令查看binglog格式；
        
        复制：mysql最重要的功能之一，高可用负载均衡和读写分离都是基于复制来实现的。从5.6开始复制有两种实现方式，基于binlog和基于GTID（全局事务标示符）
            过程：
                 1.master将数据改变记录到二进制日志(binary log)中
                 2.Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容
                 3.Master接收到来自Slave的IO进程的请求后，负责复制的IO进程会根据请求信息读取日志指定位置之后的日志信息，返回给Slave的IO进程。 返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置
                 4.Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的 bin-log的 文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master从某个bin-log的哪个位置开始往后的日志内容
                 5.Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行
    
        恢复：
             1.首先，看下当前binlog位置 show master status;
             2.向表tb_person中插入两条记录：
             3.记录当前binlog位置：show master status;
             4.查询数据 
             5.删除一条: delete from tb_person where name ="person_2";
             6.binlog恢复（指定pos点恢复/部分恢复）
                 mysqlbinlog   --start-position=1847  --stop-position=2585  mysql-bin.000008  > test.sql
        		 mysql> source /var/lib/mysql/3306/test.sql
             7.数据恢复完成 
           恢复，就是让mysql将保存在binlog日志中指定段落区间的sql语句逐个重新执行一次而已


 MYSQL权限：
		--回收建表、改表、删表、查表权限
		REVOKE CREATE ON T FROM user2；
		REVOKE DROP ON T FROM user2；
		REVOKE ALTER ON T FROM user2;
		REVOKE SELECT ON T FROM user2；

		--回收表记录的增删改权限
		REVOKE INSERT ON T FROM user2；
		REVOKE UPDATE ON T FROM user2；
		REVOKE DELETE ON T FROM user2；
	
		--查看权限
		SHOW GRANTS FOR user2;


数据系统的码：
        码：指的是能够标识实体的属性，是整个实体集拥有的性质，不是单个实体拥有的。包含超码、主码、候选码。
        超码：是一个或多个属性的集合，这些属性可以让我们在一个实体集中唯一地标识一个实体。如果K是一个超码，那么K的任意超集也是超码，也就是说如果K是超码，那么所有包含K的集合也是超码。
        候选码：从超码中选出的，自然地候选码也是一个或多个属性的集合。
        主码：被数据库设计者选中的，用来在同一实体集中区分不同实体的候选码；此外，应该选择哪些从不或极少变化的属性；


mysql的charindex：
     查找是否包含某子串的函数，charindex(‘A‘,ename)如果存在返回索引位置，该索引是从1开始的，如果不存在则返回0

数据库表结构修改：
     修改表结构包括：增加字段、删除字段、增加约束、删除约束、修改缺省值、修改字段数据类型、重命名字段、重命名表等。这些操作都是用 alter table 命令来完成。常用用法如下：
				1、增加字段：ALTER TABLE 表名 ADD 字段名 字段类型；
				2、删除字段：ALTER TABLE 表名 DROP COLUMN 字段列名；
				3、增加约束：ALTER TABLE 表名 ADD CHECK(字段名<>'')或者 ALTER TABLE 表名 ADD CONSTRAINT 约束名 UNIQUE(字段名)；
				4、删除约束：ALTER TABLE 表名 DROP CONSTRAINT 约束名；
				5、修改字段缺省值：ALTER TABLE 表名 ALTER COLUMN 字段名 SET DEFAULT 默认值；
				6、 修改字段数据类型：ALTER TABLE 表名 ALTER COLUMN 字段名TYPE l类型；
				7、重命名字段：ALTER TABLE 表名 RENAME COLUMN 旧字段名TO 新字段名；
				8、重命名表：ALTER TABLE 表名 RENAME TO 新表名。


MYSQL一条更新语句经历了什么？
     1⃣️与查询流程一样，update的时候也会经历客户端、连接器、分析器、优化器、执行期操作存储引擎。不同的是update还涉及到日志模块(redolog重做日志 和 binlog归档日志)
     2⃣️redolog: 1.类似一个速记本，先讲内容记录在这个速记本上然后在一个空闲的时候再写入汇总本上面去。这个过程称为WAL(Write-Ahead Logging),关键点就是先写日志，再写磁盘。
                 2.有了redolog之后，数据库服务发生重启事件，也可以保证之前的记录不会丢失，这个能力称为crash-safe。
                 3.该日志位于innodb引擎内。
                 4.redolog是循环写有空间限制的，当写满了之后，需要清理空间指针再跑回开头去继续写。因此没法保留很久的历史日志，也就不能起到完整归档的作用了。
                 5.**数据最终的落盘不是由redolog更新过去的，redolog只对崩溃恢复后丢失了更新的内存数据页做修改，修改完之后由另外的后台线程去写盘。

     3⃣️binlog: 1.与redo不同的是，该日志模块位于server层，所有引擎都可以实现。statement格式的binlog，最后会有commit；row格式的binlog最后会有一个xid event。
                2. binlog只能支持归档，并没有崩溃恢复的功能。对于binlog来说如果损失了数据页，旧事务的细节是补不回来的。***
     4⃣️两阶段提交：针对redolog和binlog保持逻辑一致而设计的。类似一个事务，redolog和binlog都写完才能结束。
                 为什么要有这个设计？
                      目的是为了数据库的事务持久性和主从备份的一致性。
                      a. 事务持久性，假如没有利用事务回滚这个的特性，由于更新修改不是直接作用到磁盘数据上的，在崩溃恢复之后就不会走日志去修改数据，这样修改结果就不会落到实处上了。
                      b. 主从备份一致性。同样的，写入binlog之后从数据库是修改成功了，但主数据库在写入binlog后崩溃了。这时候主库的磁盘数据其实是还没修改成功的，没有利用事务回滚那主库就认为磁盘数据就是最新的了，就不会走日志去修改数据了。
    
     5⃣️crash-safe:redolog和binlog有一个共同的数据字段xid。崩溃恢复的时候会按顺序扫描redolog:
                          a. 如果碰到既有prepare、又有commit的redolog则直接提交
                          b. 如果碰到有prepare,但是没有commit的redolog， 就拿xid去binlog找对应的事务。

SQL语句为什么变慢？
     背景：在某些时刻更新查询的sql语句会突然变慢，出现抖动的情况。这个抖动的时刻很有可能mysql是在执行flush
     flush：意思是刷脏页。脏页表示内存页与磁盘对应的数据页的内容不一致。内存数据写入到磁盘后，内存和磁盘上的数据页的内容一致了，就称内存为干净页。
     
     可能出现的场景：1⃣️redolog满了。这时候系统会停止所有更新的操作，把checkpoint往前推进，以便redolog有空间继续写。checkpoint推进的过程，需要把对应脏页都flush到磁盘上。
                   2⃣️系统内存满了，此时需要淘汰一部分数据页，如果淘汰的是脏页，就要先将脏页写到磁盘中去。
                         刷脏页一定会写盘保证了数据页两种状态：a.内存里有直接读
                                                          b.内存里没有读出来直接能用
                   3⃣️空闲的时候刷脏页
                   4⃣️正常关闭数据库的时候，会把内存中所有的脏页都flush到磁盘上。
    
     对性能的影响：1⃣️redolog满了：系统不能接受更新了，所有更新必须堵塞住。
                 2⃣️buffer pool不够用：
                         buffer有三种状态分别是没使用、干净页、脏页。此外innodb的策略是尽量使用内存，因此对于一个长时间运行的库当有写入操作的时候必然存在着刷脏页这个动作。
                         当查询要淘汰的脏页太多会导致查询响应时间明显变长。
    
     刷脏页控制策略：1⃣️innodb_io_capacity：该参数会告知innodb磁盘能力，建议设置成磁盘的iops。可以通过fio工具测出。
                                         该参数设置小的时候会造成刷脏页速度特别慢，甚至比脏页生成的速度还要慢，这样会造成脏页累积，影响了查询和更新性能。
                   2⃣️innodb_max_dirty_pages_pct：脏页比例上限，默认75%。会通过算法与innodb_io_capacity转换出刷脏页的速度。
                   3⃣️innodb_flush_neighbors:将相邻的脏页也刷掉，对于机械硬盘很有用减少了随机io。但是ssd这种io能力较强的设备就没必要用了。默认是0不开启。

MYSQL计算总行数特别慢是为什么？
     背景：在某些业务下，我们需要展示某张表的总行数。但是会发现随着数据量越来越多，查询总记录数会越来越慢。
     原因：当使用了innodb引擎，由于事务mvcc的存在，每个数据行都存在着多个版本号，因此没办法像myisam引擎那样记录一个总的记录数。因此innodb必须得扫描全表去计算得到总记录数。另外还有一种得到总记录数的方法是通过show table status，但这个数据是通过对索引采样统计出来的结果存在40%-50%的误差因此没法直接用于业务上面。

     解决方案：1⃣️通过缓存系统例如redis存取表总记录数这个值
                存在问题：a.由于系统异常重启这个值可能会丢失，但通过重新扫描全表再把记录回写到缓存系统可以解决一般问题不大
                         b.同步问题：无论是先在redis里增加记录数再往数据表添加数据，或者颠倒操作都可能在查询的时候出现缓存系统记录数和数据表不同步。这种一般在线上是无法忍受的必须通过加锁或者利用mysql的事务去将这两个行为绑定在一起操作完之后再查询才能保持一致
              2⃣️在数据库中保存计数
                 a.可以解决系统重启数据丢失问题，innodb支持崩溃恢复不丢失数据
                 b.通过事务把计数值+1和插入一行数据都纳入到一起。innodb在可重复读的级别下，查询计数值和对应数据行逻辑上应该是一致的。事务不提交，在其他会话中查询还是原先的值。


MYSQL按字段排序
     背景：在开发应用的时候，经常需要根据指定字段去排序。执行语句是如何排序会影响到整体的效率
     根据字段数量区分排序：
          当执行一个语句需要排序的时候，mysql会给线程分配一块内存sort_buffer用于排序。如果只需排序字段加主键id放入sort_buffer则称为rowid排序；如果是全字段加入sort_buffer则称为全字段排序。

          全字段排序：
                  流程：1.初始化sort_buffer，放入查询的所有字段
                       2.从查询普通索引树中找到第一个满足条件主键id
                       3.用该主键id从主索引树中取出整行，然后取查询需要的字段放入sort_buffer
                       4.从普通索引树取下一个主键id
                       5.从普通索引树中重复3、4步骤直到不满足查询索引条件
                       6.对sort_buffer按照指定排序字段做快速排序
                       7.按照排序结果取指定行或所有行给客户端
                   sort_buffer_size: sort_buffer的大小，如果太小放不下全部数据则利用磁盘临时文件，把数据放入到多个临时文件中去。利用这些临时文件进行归并排序
          rowid排序：
                  流程：1.初始化sort_buffer，只放入两个字段排序字段和id
                       2.从查询普通索引树中找到第一个满足条件的主键id
                       3.用该主键id从主索引树中取出整行，然后取排序字段和id放入sort_buffer
                       4.从普通索引树取下一个主键id
                       5.从普通索引树中重复3、4步骤直到不满足查询索引条件
                       6.对sort_buffer按照指定排序字段做快速排序
                       7.遍历排序结果，按照id值回道原表取出需要查询的字段返回给客户端
                   max_length_for_sort_data: 专门控制用于排序的行数据的长度的一个参数。如果单行的长度超过了这个值则mysql认为单行太大需要换个算法即用rowid排序。否则的话是用全字段排序
    
           异同点：
                内存使用：相对于全字段排序，rowid排序是在内存不足的情况下才使用的。rowid排序比全字段多了一步回表操作导致了需要多扫描一遍原表。
                        **体现了mysql的一个设计思想如果内存够则尽量利用内存，去减少磁盘访问
    
      优化：1⃣️mysql做排序是个成本比较高的事情，扫描的行数至少是指定普通索引对应的行数。如果对于经常需要排序的字段可以考虑通过建立联合索引少这种临时表。
           2⃣️利用索引树叶子结点会排序的特点，从左至右按查询条件字段---排序字段建立联合索引。
           3⃣️甚至于可以利用覆盖索引原理，即索引上的信息足够满足查询请求，就不需要再回到主键索引上去取数据了。把需要查询的字段也加入到联合索引的右边，这样就不用回表到主索引树上面去查询，直接在普通索引树把数据捞出来。
           4⃣️利用覆盖索引的话需要耗费更多资源去维护

MYSQL随机取字段内数据
     背景：业务中经常需要随机展示数据库的一些信息，应该避免使用order by rand()来实现实时返回任务。

     临时表排序： order by rand()需要使用临时表，并且需要在临时表上面做排序。这里的随机取数据本质是生成表长度个随机数，然后对随机数排序选取派在前面几个随机数对应的位置信息然后到原表去拿数据。计算过程复杂，通常是不用的。
    
     随机排序方法：1⃣️用随机数通过公式生成一个表主键id最大值和最小值中间的数，然后取不小于这个数的第一个id的行。
                   缺点：每行数据给取到的概率不均匀，假如主键id之间有大量的空洞那就出问题了。
                 2⃣️用随机数乘以表长度然后取整数部分作为limit起始下标, 然后取下一行。
                        set @Y=floor(@C * rand());
                        select * from t limit @Y, 1;

mysql同样逻辑的语句性能差异巨大
     背景：在mysql中，有很多逻辑看上去相同，但性能差异巨大的语句，对这些语句使用不当的话，就会不经意间导致整个数据库的压力变大。
     1⃣️条件字段函数操作
           在where后面的条件字段包裹一层函数，无论该函数是否会改变原条件字段在索引树上面的有序性，优化器都不会再考虑使用索引了。主要原因是b+树同一层的兄弟节点天然就是有序的，对索引字段增加了函数操作，会破坏这种有序性，导致在树上无法进行查询
     2⃣️隐式类型转换
           如果查询条件字段本身是类型是字符串类型，但在查询语句中给定的查询条件是数字类型。由于mysql在字符串与数字类型做匹配时会将字符串转换为数字类型，那么等于在查询条件上面增加了一层类型转换函数。因此会改变了索引字段的有序性导致全表扫描。
     3⃣️隐式字符编码转换
           当两张表做关联查询的时候尽量保持CHARSET是一致的。
           假如驱动表是字符集是utf8mb4，被驱动表的字符集是utf8。由于utf8mb4是utf8的超集，因此当查询语句执行到被驱动表时，被驱动表字段会包裹一层函数将其转换为utf8mb4。这一步相当于又是对条件字段进行函数操作导致失去了索引字段的有序性，从而去全表扫描。由于驱动表与被驱动表字符集的不同导致了全表扫描，但是当被驱动表的字符集是驱动表的超集的时候，convert函数就会加在被驱动表查询的输入函数上面，这时候就可以用上被驱动表的索引了。
           驱动表：联合查询中第一个取条件字段的表； 被驱动表：与驱动表的字段进行关联查询的表

     总结：由于对索引字段进行了函数操作，可能会破坏索引值的有序性，因此优化器会放弃走树搜索功能。在业务新增sql语句的过程中，先explain一下查看一下扫描数看看有没有改进的可能。

MYSQL单查询语句只查询一行确很慢（总结）
      背景：在有些情况下，不是很复杂的sql语句如只有“查一行”也会执行的特别慢或者干脆卡住长时间不返回。并且这个慢是排除了cpu占有率io利用率很高的情况，通常是由于锁或者是事务导致的
      查询各线程在做什么：show processlist;
      长时间不返回：1⃣️整个表被另一个线程给锁住了，通常解决方案就是找到谁持有mdl写锁，然后kill掉
                  2⃣️等flush:通常对一张表或全部表做flush操作很快就执行完，假如给堵住了通常是因为前面还有一个线程在占据着表导致flush操作没办法去关闭表，从而flush就会堵住后面的查询了。
                  3⃣️等行锁：启动了两个事务，一个事务先占有某行数据写锁并且不提交，那在另一个事务中就一直无法读这行数据了
      查询慢：1⃣️在不是索引的上的字段上查询，此时通过主索引慢慢通过主键顺序进行扫描，表越长查询的越慢通常是线性增加的。
             2⃣️在是索引的字段上查询的慢：
                            通常是由于事务的一致性读引起的。开启两个事务a,b，并在事务a开头开启快照。事务b对数据进行了上百万次的更新，这些更新写回滚日志到undo log日志里面去。目的是为了在事务a进行一致性读的时候能够保持一致，这里的保持一致的方法就是通过把当前读的数据进行上百万次的回滚操作。因此会发现一致性读的查询速度变得很慢，而全局的当前读速度正常。


MYSQL间隙锁和nextkeylock加锁原则总结:
      背景：当在可重复读的隔离环境下，为了防止幻读引入了间隙锁和next-key lock的概念。间隙锁配合上行锁以后很容易在判断是否会出现锁等待的问题上犯错。
      原则总结：1⃣️原则1:加锁的基本单位是next-key lock。前开后闭区间
               2⃣️原则2:查找过程中访问到的对象才会加锁
               3⃣️优化1: 索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁
               4⃣️优化2: 索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。在间隙锁里面有数据不能update，没数据的不能insert
               5⃣️一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止**（该bug在较新的8.0.12版本中已经不存在了）

      现实案例：1⃣️等值查询间隙锁：
                       触发条件：如果索引是唯一索引，则查询的目标值在表中不存在才会触发间隙锁，否则存在的话会变成行锁；如果是普通索引无论如何都会变成间隙锁
               2⃣️非唯一索引等值锁：
                       肯定会变成间隙锁，但主键或其他字段可能不加锁，区别是是否使用了覆盖索引。因为只有访问到的对象才会加锁，如果查询使用了覆盖索引，并不需要访问主键索引，所以主键索引不会加任何锁。
               3⃣️主键索引范围锁
                       查询范围内可能包含多个间隙，并且间隙间存在多个行锁。因此一个主键索引范围查询假的锁包括了左右两端的间隙，和这两个间隙包裹着中间的所有行锁和间隙锁。
               4⃣️非唯一索引范围锁
                       同主键索引范围锁差不多，不同的是最右边不再是间隙锁而是next key lock包含了一个行锁。
               5⃣️非唯一索引上存在着“等值”
                       与非唯一索引等值锁加锁的思路类似。多个等值之间由于主键之间是存在间隙的，因此两个等值之间是有间隙的，锁上的是等值之间主键的间隙，而对于查询的非唯一索引来说锁上的是等值左右两边的间隙和等值对应的行锁。通常这个情况在delete删除数据的时候出现，这时候可以通过limit限制一下要删除的条数那么就可以少加一些间隙锁了。
               6⃣️死锁
                      next-key lock实际上是间隙锁和行锁加起来的结果，因此实际分析锁的时候要清楚等到引擎具体执行的时候，是会分成间隙锁和行锁来分开执行的。
                      例如两个会话a,b：会话a开启事务，做等值查询加上了等值对应的行锁和两边的间隙锁。会话b更新该等值的行数据，此时先加等值左边的间隙锁，然后加行锁，由于会话a还没提交事务因此会话b会在行锁上面卡住。然后会话a紧接着执行了插入操作，插入的数据刚好在会话b间隙锁上面，导致了会话a没办法去commit，那就出现了死锁了。

在线上过程中遇到业务高峰期临时提高性能的方法有哪些？
       背景：在业务高峰期，生产环境的mysql压力过大，没法正常响应，需要短期内、临时性地提升一些性能
       问题：1⃣️短连接风暴。正常的短链接模式就是连接到数据库之后，执行很少的sql语句就断开，下次有需要的时候再重连。如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。
                    解决方案(都是有损的，通常都是dba去执行)：
                                1.先处理掉那些占着连接但不工作的线程
                                        a.wait_timeout: 设置该参数表示，一个线程空闲了wait_timeout这么多秒之后，就会被mysql直接断开连接
                                        b.kill_connection: 主动的kill掉线程。可以通过show processlist查询处于sleep状态的线程，并且通过information_schema.innodb_trx表先查询是否处于事务中。先kill掉那些不处于事务中的线程，然后再kill掉那些处于事务中的线程
                                2.减少连接过程的消耗
                                        风险极高的方案，即让数据库跳过权限验证阶段。不建议使用

            2⃣️慢查询性能问题。
                    解决方案：
                         1.索引没有设计好
                                   可以通过online DDL去alter table。一般比较理想的是先从备库去执行。
                                         a.主库为a,备库为b。在备库b上面执行 set sql_log_bin=off,不写binlog。然后执行alter table语句加上索引
                                         b.执行主备切换 ***
                                         c.这时候主库是b，备库是a。在a上执行 set sql_log_bin=off, 然后执行alter table加上索引
                         2.语句没写好
                                 由于sql查询语句没写好，例如在索引字段上增加了函数操作导致了不能使用索引查询。
                                 解决方案是临时使用query_rewrite，往query_rewrite.rewrite_rules这个表里面增加语句改写规则，并通过call query_rewrite.flush_rewrite_rules();生效
                         3.选错索引
                             给语句加上force index
            3⃣️QPS突增问题
                   由于缓冲给击穿、业务出现高峰、或者应用程序bug，导致某个语句的qps突然暴涨，也可能导致mysql压力过大，影响服务。
                   解决方案：1. 如果是个新功能的bug，可以先把业务下掉
                                      下掉的几种方式：a.从数据库端直接把白名单去掉
                                                    b.如果新功能使用的是单独的数据库用户，直接把该用户删掉，这样连接就不成功了。
                                                    c.如果功能是跟主体功能混在一起查询，可以通过处理语句来限制。通过语句重写功能，把压力最大的sql语句直接重写成"select 1"返回。尽量不用
    
        预先发现问题：
              1. 上线前，在测试环境，把慢查询日志(slow log)打开，并且把long_query_time设置成0，确保每个语句都会被记录入慢查询日志。
              2. 在测试表里插入模拟线上的数据，做一遍回归测试
              3. 观察慢查询日志里每类语句的输出，特别留意rows_examined字段是否与预期一致。
              4. 如果新增的sql语句不多，手动跑一下即可。而如果是新项目，或者是修改了原有项目的结构设计，全量回归测试是必要的。
        
        总结: 
              1.在实际开发过程中，尽量避免大量地使用短连接
              2.连接异常断开是常有的事，代码里要有正确地重连并重试的机制。


*MYSQL是怎么保证数据不丢失的？
       背景：只要redolog 和binlog保证持久化到磁盘中，就能确保mysql异常重启后，数据可以恢复。如何保证redo log真实地写入了磁盘。
       1⃣️binlog的写入机制
            binlog cache:每个线程一个，通过binlog_cache_size限制大小，超过了要暂存到磁盘。

            写入逻辑：事务执行过程中，先把日志写到binlog cache。一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。事务提交的时候，执行期把binlog cache里的完整事务写入到binlog中，并清空binlogcache
    
            binlog cache ---> binlog file :
                  1.分两步分别是: a. write:指的是把日志从binlog cache写入到page cache，并没有把数据持久化到磁盘，速度较快
                                b. fsync:该方法真正的落到磁盘上去
                  2. 通过sync_binlog控制write和fsync时机。该参数为0时表示只write; 为1时每次都执行fsync; >1的时候使用了装桶的方法，表示每次事务都只write,等累计到N个事务才fsync，该方法在主机异常重启的时候是会有损的桶里的事务会丢失。
    
       2⃣️redolog的写入机制
            redolog buffer: 事务在执行过程中，生成的redolog是要先写到redolog buffer。 redolog buffer里面的内容不是每次都要直接持久化到磁盘上，如果事务执行期间异常重启，那么直接丢失掉不会有任何损失。同时在事务执行期间也有可能部分日志被持久化到磁盘上去。
    
            redolog的三种状态：1.redolog buffer:物理上是在mysql的进程内存中
                             2.写到磁盘(write)，但是还没有持久化(fsync)，物理上是在文件系统的page cache(也是一种WAL机制)里面
                             3.进行了fsync操作，持久化到了硬盘。
    
            控制三种状态的参数： innodb_flush_log_at_trx_commit： 0表示提交事务时候只把redolog留在redolog buffer中
                                                               1表示提交事务时候直接将redolog持久化到磁盘
                                                               2表示提交的时候只把redolog写到page cache
    
            redolog持久化到磁盘可能的场景： 1.innodb有一个后台线程，每隔一秒会执行一次调用write把redolog buffer日志写到page cache,然后再调用fsync持久化到磁盘。
                                         2. redolog buffer占用的空间达到innodb_log_buffer_size参数的一半，后台线程也会主动写盘。此时只写到page cache
                                         3. 并发的事务提交，会将这个事务的redolog buffer中的其他事务也持久化到磁盘。
    
       3⃣️双 “1”配置
            概要：指的是sync_binlog和innodb_flush_log_at_trx_commit这两个参数都设为1.
            作用：都设为1意味着一个事务在完整的提交之前，需要等待两次刷盘，一次是redolog(prepare阶段)，一次是binlog
    
       4⃣️降低磁盘iops消耗
            组提交机制：把日志积累到一定程度再一块同时写到磁盘，减少磁盘寻道的资源浪费。
                      LSN日志逻辑序列号：单调递增，用来记录redolog的累积长度。通过该标识累积多个事务，同时调用fsync将redolog buffer做持久化
            HOW：redolog、binlog分别依次执行write。然后redolog在binlog执行write的中间累积事务，binlog执行完write，redolog就调用fsync。同理binlog在redolog调用fsync期间累积多个事务。
    
       5⃣️wal机制的好处
            1. redolog和binlog都是顺序写，磁盘的顺序写比随机写顺序要快（少了寻道的时间）
            2. 组提交机制，可以大幅度降低磁盘的iops消耗

MYSQL是怎么样保持主备一致的？
      背景：mysql通过binlog来进行归档，并且进行主备同步实现了高可用
      1⃣️mysql主备的基本原理
              主备切换流程：   
                      状态1:       client
                                     |
                                     |
                                     |
                                   MysqlA---- MysqlB(Read Only)
                                   
                      状态2:                            client
                                                          |
                                                          |
                                                          |
                                 MysqlA(Read Only)----MysqlB 
    
              建议将备库都设为只读模式： 1.线下的查询语句到备库上面查询防止误操作
                                     2.防止切换逻辑有bug，比如切换过程出现了双写造成主备不一致
                                     3.可以用readonly状态，判断节点的角色
      2⃣️备库b和主库a之间发生了什么
              一个长连接：1.在备库b上面通过change master命令，设置主库a的ip、端口、用户名、密码，以及要从那个位置开始请求binlog，这个位置包含文件名和日志偏移量
                        2.在备库b上面执行start slave命令，这时候备库会启动两个线程分别是io_thread和sql_thread。其中io_thread负责与主库建立连接
              binlog传输日志：1.主库a校验完用户名密码后，开始按照备库b传过来的位置，从本地读取binlog，发给b
                             2.在备库b拿到binlog后，写到本地文件称为中转日志(relay log)
                             3.sql_thread读取relaylog，解析出日志里的命令，并执行
    
      3⃣️binlog的内容
              1⃣️statement格式： 第一行：SET@@SESSION.GTID_NEXT='ANONYMOUS' 主备切换的时候用到
                               第二行：BEGIN 开启一个事务
                               第三行：指定库表+真实sql数据
                               第四行：commit提交事务并且带上xid
                 
                 优势：一个sql语句被记录到binlog不占空间
                 缺陷：如果删除数据的时候带limit，由于主库使用索引可能不一致导致删除的行数不一致从而导致了主备数据不一致
              
              2⃣️row格式：第一行：SET@@SESSION.GTID_NEXT='ANONYMOUS' 主备切换的时候用到
                         第二行：BEGIN 开启一个事务
                         第三行：Table_map event, 用于说明接下来要操作的库表
                         第四行：DELETE_ROWS(或其他update之类) envent，用于定义行为并指定行
                         第五行：commit提交事务并且带上xid
                 优势：当binlog_format使用row格式的时候,binlog里面记录了真实删除行的主键id，这样binlog传到备库的时候就可以避免索引选择错误导致主备数据不一致的现象。
                 缺陷：要把一条语句涉及到的所有操作记录都写到binlog中，占用了空间和消耗了io资源影响执行速度。
              3⃣️mixed格式：混合了row和statement，当涉及到可能索引不一致的情况是系统自动使用row格式。
                          线上环境的mysql至少要使用后mixed格式。
    
              对于大部分场景来说肯定是选用row格式：
                          1. 带来直接的好处是恢复数据。            
                             例如：a.删除数据，只要把binlog里的语句delete改为insert即可
                                  b.插入数据错误。只要把binlog里的insert改为delete
                                  c.更新数据错误。只要把event前后的两条信息对调即可
                          2.标准恢复数据的做法。 
                                  用mysqlbinlog工具解析出来，然后把解析结果整个发给mysql执行
    
        4⃣️循环复制问题
              背景：在生产环境中通常用的不单单是M-S结构，实际生产中使用的是双M结构。双M结构在binlog同步时有一个问题，主库将binlog传输给备库单此时备库也作为主库，因此binlog可能会在两个库之间无限循环复制。
              解决方案：1.规定两个库的server id必须不同，如果相同则不能设定为主备关系。
                       2.一个备库接到binlog并在重放过程中，生成与原binlog的serverid相同的新的binlog
                       3.每个库在收到从自己的主库发过来的日志后，先判断serverid，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志

mysql是如何在主备一致的基础上实现高可用？
        背景：正常情况下，只要主库执行更新生成的所有binlog，都可以传到备库并且被正确执行，备库就能达到跟主库一致的状态，这就是最终一致性。但是mysql要提供高可用的能力，只有一致性是不够的。
        1⃣️主备延迟：同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值。
                   可通过show slave status返回结果里面会显示seconds_behind_master
                   主备延迟的主要来源是备库接收完binlog和执行完这个事务之间的时间差。主备延迟最直接的表现是，备库消费中转日志relay log的速度比主库生成binlog的速度要慢

          主备延迟的来源： 1.备库所在的机器性能要比主库差很多， 现在由于主库可能发生切换，备库随时可能变成主库，所以主备库选用相同规格的机器
                         2.做了主备对称部署之后，由于备库的压力大依然会造成主备不一致。在备库上面跑大量的运营后台的分析语句。
                                  解决方案：a.一主多从。除了备库之外，横向分库，多接几个从库，让这些从库来分担读的压力。作为数据库系统，还必须保证有定期全量备份的能力。
                                          b.通过binlog输出到外部系统，让外部系统提供统计类查询的能力
                         3.大事务：如果主库上面一个事务执行了10分钟，那么备库上面也会执行10分钟就会导致延迟了。
                                  a.删除历史数据:业务在删除历史数据的时候，尽量将一个事务分成多个事务数据量，分成多次删除。
                                  b.大表DDL
    
        2⃣️主备切换的策略： 1.可靠性优先策略：
                         2.可用性优先策略：可能会导致主备不一致


mysql的一主多从结构
        背景：大多数的互联网应用场景都是读多写少，因此业务很可能遇上读性能的问题
        结构：一主多从结构通常都有一个互为主备的节点，然后有很多个从库指向其中的主库。主库负责所有的写入和一部分读，其他的读请求则由从库分担。一主多从的的主备切换相对于只有主备节点来说更加复杂，多个从库指向新的主库节点多了很多步骤
        方法：
          主库A; 备库A' ; 从库三个B、C、D
         1. 当把节点B设置成节点A'的从库时候，经历了以下：
                     CHANGE MASTER TO
                     MASTER_HOST=
                     MASTER_PORT=
                     MASTER_USER=
                     MASTER_PASSWORD=
                     MASTER_LOG_FILE=
                     MASTER_LOG_POS=
                 以上设置前面四个参数很明显就是平时经常用到的ip端口账号密码等。
                 最后两个参数表示从库要从主库的MASTER_LOG_FILE文件和日志下MASTER_LOG_POS这个同步位点继续同步，即主库对应的文件名和日志偏移量。

         2.找"同步位点"的方法
               1⃣️考虑到切换过程中不能丢失，因此在找位点的时候，尽量找一个稍微靠前，然后通过判断跳过那些在从库B上面已经执行过的事务
                         a. 等待新主库A'把relay log全部同步完成
                         b.在A'上执行show master status得到最新的file和position
                         c.取原主库A故障的时刻T
                         d.用mysqlbinlog工具解析A'得到时刻T的位点
                 对于找到的位点此时可能并不准确，例如该位点的数据此时可能已经在之前的主库A写入A'和B，因此再次从A'同步到B很可能会提示主键冲突等情况。因此还需要跳过错误
                         a.第一种方法是主动跳过一个事务
                                  set global sql_slave_skip_counter=1;
                                  start slave;
                         b.第二种方法是通过设置slave_skip_errors，直接设置跳过指定的错误
                                  在执行主备切换的时候有两类错误的时候经常遇到
                                          ·1062错误是插入数据时候唯一键冲突
                                          ·1032错误是删除数据时候找不到行
                                  注意事项：
                                       必须确认跳过错误是无损的，另外稳定了之后及时把该参数设空
    
               2⃣️GTID
                   背景：第一种通过跳过或忽略错误的方法操作复杂并且容易出错。所以mysql5.6版本引入了GTID，彻底解决了这个问题
                   什么是GTID?
                        全称是Global Transaction Identifier,全局事务ID。是一个事务在‘提交’的时候生成的，是这个事务的唯一标识。
                        
                        组成格式：GTID=server_uuid:gno
                                server_uuid是实例第一次启动时自动生成的，是一个全局唯一的值。
                                gno是一个整数，初始值是1，每次提交事务的时候会分配给这个事务，并+1。跟前面的transaction_id需要做区分，transaction_id即事务id是在事务执行过程中分配的，如果这个事务回滚了，事务id也会递增。而gno是在事务提交的时候才会分配的。
    
                        启动方式：启动mysql实例的时候，设置参数grid_mode=on和enforce_gtid_consistency=on就可以了
    
                        GTID生成方式：有两种生成方式，取决于session变量gtid_next的值
                                        a.如果gtid_next=automatic,代表使用默认值。
                                             ·记录binlog的时候，先记录仪一行SET @@SESSION.GTID_NEXT=`server_uuid:gno`;
                                             ·把这个GTID加入到节点的GTID集合
                                        b.如果gtid_next是一个指定的值，比如通过set gtid_next='current_gtid'指定为current_gtid
                                             ·第一种可能的情况是current_gtid已经存在GTID集合里面，接下来执行的这个事务会直接被系统忽略
                                             ·第二种可能是current_gtid没有存在GTID集合里面，那就将current_gtid分配给接下来要执行的事务，此时gno不用再+1。
                                           注意事项：
                                                一个current_gtid只能给一个事务使用，事务提交之后gtid_next要重新设置
    
                        用法：
                            在gtid模式下，备库B设置成节点A'的从库时候有点不一样：
                                     CHANGE MASTER TO
    			                     MASTER_HOST=
    			                     MASTER_PORT=
    			                     MASTER_USER=
    			                     MASTER_PASSWORD=
    			                     master_auto_position=1
    			            master_auto_position表示主备关系使用的是GTID协议，不用再去管位点了。
    			            
    			            此时在实例B执行start slave命令，取binlog的逻辑：
    			                  实例A'的GTID集合记为set_a;实例B的GTID集合记为set_b
    			                1.实例B指定主库A'，基于主备协议建立连接
    			                2.实例B把set_b发给主库A'
    			                3.实例A'算出set_a与set_b的差集，也就是所有存在set_a但是不存在set_b的GTID的集合。然后判断A'本地是否包含了这个差集需要的所有binlog事务。
    			                     如果不包含，表示A'是落后B的，直接返回错误。
    			                     如果包含，表示B是A'的子集，因此找出第一个不在set_b的事务发给B.
    			                4.之后就从这个事务开始，往后读文件，按顺序取binlog发给B去执行。
    
    			            设计思想：新主库需要判断日志的完整性。新主库的日志必须领先于从库。


MYSQL一主多从的应用场景
      背景：由于主从之间不可避免出现延迟，因此很可能会在从库上面读到一个过期的数据。
      处理过期读的几种方案：
         1⃣️强制走主库方案：
               逻辑：将查询请求进行分类，对于必须要拿到最新结果的强制发到主库上进行请求。对于可以读到旧数据的请求，才将其发到从库上面。
               存在问题：当所有查询都不能是过期读，例如金融类的业务，那么此时必须把读写压力都放到主库上面，等于放弃了扩展性
         2⃣️sleep方案
               逻辑：主库更新之后，读从库之前先sleep以下再拿结果。例如前端新增数据操作请求发完之后直接把输入的内容显示在页面上，而不是真正的去数据库去捞插入的数据。
               存在问题：延迟多长很难判断。
         3⃣️判断主备无延迟方案
               逻辑：通常有三种做法：
                          a.通过show slave status结果里的seconds_behind_master参数判断是否为0
                          b.通过比对位点主备的值是否相等
                          c.通过对比GTID集合确保主备无延迟
               存在问题：判断的无延迟依旧有可能出现问题，例如有判断的时候刚好有一个新的事务进来但还没加入从库里面此时判断无延迟那就查不到了。
         4⃣️判断主备无延迟配合semi-sync
               逻辑：解决无延迟判断可通半同步复制来解决。semi-sync配合前面关于位点的判断，就能够确定在从库上执行的查询请求，可以避免过期读。
                    1.事务提交的时候，主库把binlog发给从库
                    2.从库收到binlog以后，发回给主库一个ack表示收到了
                    3.主库收到这个ack之后，才能给客户端返回“事务完成”的确认
               存在问题：1. 这个配合方式只适合一主一备的场景，在一主多从的情况下如果查询落到其他从库上还是可能没收到最新日志导致过期读。
                        2.在业务高峰期，主库的位点orGTID集合更新很快，那么上面的位点查询就会一直不成立，很可能出现从库上迟迟无法响应查询请求。
         5⃣️等主库位点方案解决 ‘判断主备无延迟配合semi-sync’存在问题
               命令语句：select master_pos_wait(file, pos[, timeout]);
                      1.该语句是在从库上看执行的；
                      2.参数file和pos指的是主库上的文件名和位置
                      3.timeout可选，表示最多等多少秒
                      4.返回一个正整数或0表示从命令开始执行到指定pos位置中间执行了多少个事务
               方案逻辑：
                     1.trx1事务更新完成后，马上执行show master status得到当前主库执行的file和position
                     2.选定一个从库执行查询语句
                     3.在该从库上面执行select master_pos_wait(file, pos[, timeout]);
                     4.如果从库上面返回值是>=0的正整数，则在这个从库执行查询语句
                     5.否则跑到主库上执行语句
               存在问题：select master_pos_wait等待时间过长通常会选择退化到主库执行查询语句，如果所有从库都延迟超过1秒，那么查询压力会突然都到主库上面。假如设定不允许过期读的时候在这种时候要么超时放弃要么转到主库上去查询。

         **6⃣️GTID方案
                命令语句：select wait_for_executed_gtid_set(gtid_set, 1);
                      1.等待，直到这个库执行的事务中包含传入的gtid_set,返回0；
                      2.超时返回1.
                方案逻辑：
                      1.trx1事务更新完成后，客户端直接从返回包获取这个事务的GTID,记为gtid1;**(需要使用内置的mysql_session_track_get_first)
                      2.选定一个从库执行查询语句；
                      3.在从库上执行select wait_for_executed_gtid_set(gtid1, 1);
                      4.如果返回值是0，则在这个从库执行查询语句；
                      5.否则，到主库执行查询语句
           
           7⃣️总结
                在实际应用中，几个方案是可以混合使用的。比如先在客户端对请求做分类，区分哪些请求可以接受过期读，而那些请求完全不能接受过期读；然后对于不能接受过期读的语句再使用GTID或等位点的方案。
                无论如何，不管什么方案如果是不需要等待就可以水平扩展数据库的方案，往往是用牺牲写操作性能来换取的，也就是需要再读性能和写性能中取权衡





